# -*- coding: utf-8 -*-
"""Fraud_check_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RN-kUo8F1D479-Ar4pBnWTimFckbe-UP
"""

#Use decision trees to prepare a model on fraud data 
#treating those who have taxable_income <= 30000 as "Risky" and others are "Good"
#Data Description :
#Undergrad : person is under graduated or not
#Marital.Status : marital status of a person
#Taxable.Income : Taxable income is the amount of how much tax an individual owes to the government 
#Work Experience : Work experience of an individual person
#Urban : Whether that person belongs to urban area or not

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#loading dataset Company_dataset
data = pd.read_csv("Fraud_check.csv")
data.head()

data.info()

#new dataframe created for preprocessing
data_new=data.drop_duplicates().copy()

data_new.info() # 600 records and 6 columns

#EDA and Data Preprocessing
#Making categorical data for Target column Taxable.Income
# 1 : Taxable.Income<=30000 Risky
# 2 : Taxable.Income>30000 Good
for i in range(0,len(data_new)):
    if(data_new["Taxable.Income"][i]<=30000):
        data_new["Taxable.Income"][i]="Risky"
    else:    
        data_new["Taxable.Income"][i]="Good"

data_new=data_new.rename({'Taxable.Income': 'TaxIn','Marital.Status':"MStatus",'City.Population':'CityPop',"Work.Experience":"WorkExp"}, axis=1)

data_new.info()

plt.hist(data_new["TaxIn"]) # imbalanced data set

data_new.TaxIn.value_counts()

#Mapping columns which are categorical to dummy variables
data_new["TaxIn"]=data_new["TaxIn"].map({"Risky":1,"Good":2})
data_new["Undergrad"]=data_new["Undergrad"].map({"YES":1,"NO":2}) 
data_new["MStatus"]=data_new["MStatus"].map({"Single":1,"Divorced":2,"Married":3})
data_new["Urban"]=data_new["Urban"].map({ "YES":1,"NO":2})

colnames= list(data_new.columns)
target=colnames[2]
colnames.pop(2)
predictors=colnames

def norm_func(i):
    x = (i-i.mean())/(i.std())
    return (x)

data_new['CityPop']=norm_func(data_new['CityPop'])

# Splitting data into training and testing data set by 70:30 ratio

from sklearn.model_selection import train_test_split
train,test = train_test_split(data_new,test_size = 0.3)

#Inorder to handle imbalance data we use smote algoritmn to balance dataset while training and testing
from imblearn.over_sampling import SMOTE 
sm = SMOTE(random_state = 2)

X_train_res, Y_train_res = sm.fit_sample(train[predictors],train[target])

X_test_res,Y_test_res = sm.fit_sample(test[predictors],test[target])

Y_train_res.shape, Y_test_res.shape

plt.hist(Y_train_res) ; plt.hist(Y_test_res);

#Decision Tree Model building and Validity checking
from sklearn.tree import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion = 'entropy')
model.fit(train[predictors],train[target])

preds = model.predict(test[predictors])
pd.Series(preds).value_counts()

# Accuracy = train  
np.mean(train.TaxIn == model.predict(train[predictors]))#1

# Accuracy = Test
np.mean(preds==test.TaxIn) # 0.61

from  sklearn.metrics import classification_report
print(classification_report(test[target],preds))# precision#0.1 and recall #0.11 for "Risky" class are very less

#After Applying Smote to test and train
model.fit(X_train_res,Y_train_res)

pred=model.predict(X_test_res)
pred

# Accuracy = train  
np.mean(Y_train_res == model.predict(X_train_res))

Y_test_res

X_test_res

# Accuracy = Test
np.mean(pred==Y_test_res) # 0.6875  Accuracy improved from smothe algoritmn

from  sklearn.metrics import classification_report
print(classification_report(Y_test_res,pred))# precision #0.7 and recall #0.65 improved for "Risky" Class

#Bagging Classifier to improve accuracy 
from sklearn.ensemble import BaggingClassifier

c = DecisionTreeClassifier()
num_trees = 200
model = BaggingClassifier(base_estimator=c, n_estimators=num_trees, random_state=5)
model.fit(X_train_res,Y_train_res)

pred=model.predict(X_test_res)

# Accuracy = train  
np.mean(Y_train_res == model.predict(X_train_res)) #1

# Accuracy = Test
np.mean(pred==Y_test_res) #0.763

print(classification_report(Y_test_res,pred)) # precision#0.83 and recall#0.66 for "Risky" class is improved

#######Boosting######
from xgboost import XGBClassifier

model= XGBClassifier()
model.fit(X_train_res, Y_train_res)

pred=model.predict(X_test_res)

# Accuracy = train  
np.mean(Y_train_res == model.predict(X_train_res)) #0.88

# Accuracy = Test
np.mean(pred==Y_test_res) #0.836

print(classification_report(Y_test_res,pred)) # high precision#0.95 and recall#0.71 for risky class

# we use XGBoosting algorithm for decision tree for fraud detection as final model