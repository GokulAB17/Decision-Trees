# -*- coding: utf-8 -*-
"""Company_data_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YlNaCAGpoXrM7vMNGKtd5hb45KHlldPY
"""

#A cloth manufacturing company is interested to know about the segment or 
#attributes causes high sale. 
#Approach - A decision tree can be built with target variable Sale
# (we will first convert it in categorical variable) & 
#all other variable will be independent in the analysis.

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#loading dataset Company_dataset
data = pd.read_csv("Company_Data.csv")

#Data Preprocessing and EDA
data.head()

data.info()
#11 columns and 3 columns containing categorical data and size is 400

# no null values in dataset
colnames = list(data.columns)
predictors = colnames[1:11]
target = colnames[0]

data["Sales"].max()#16.27

data["Sales"].min() #0.0

data["Sales"].median() #7.49

plt.hist(data["Sales"]) #normally distributed

#new dataframe created for preprocessing
data_new=data.drop_duplicates().copy()
data_new.info()

#Making categorical data for Target column Sales in 3 categories
# 1 : Sales<=5
# 2 :5>Sales<=10
# 3 : Sales>10
for i in range(0,len(data)):
    if(data_new["Sales"][i]<=5):
        data_new["Sales"][i]="<=5"
    elif(data_new["Sales"][i]<=10 and data_new["Sales"][i]>5):
        data_new["Sales"][i]="5>s<=10"
    else:    
        data_new["Sales"][i]=">10"

data_new.Sales.value_counts()  # imbalance dataset

#Mapping columns which are categorical to dummy variables
data_new.Sales=data_new.Sales.map({"<=5":1,"5>s<=10":2,">10":3})
data_new.ShelveLoc=data_new.ShelveLoc.map({"Bad":1,"Good":3,"Medium":2})        
data_new.Urban=data_new.Urban.map({"Yes":1,"No":2})
data_new.US=data_new.US.map({"Yes":1,"No":2})

plt.hist(data_new["Sales"])

# Splitting data into training and testing data set by 70:30 ratio

from sklearn.model_selection import train_test_split
train,test = train_test_split(data_new,test_size = 0.3)

#Decision Tree Model building and Validity checking
from sklearn.tree import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion = 'entropy')
model.fit(train[predictors],train[target])

preds = model.predict(test[predictors])
pd.Series(preds).value_counts()
pd.crosstab(test[target],preds)

# Accuracy = train  
np.mean(train.Sales == model.predict(train[predictors]))#1

# Accuracy = Test
np.mean(preds==test.Sales) # 0.675

#Accuracy is less as model is biased towards majority class as data set is imbalanced

from  sklearn.metrics import classification_report

print(classification_report(test[target],preds))# precision and recall is less for minority classes

#Bagging Classifier to improve accuracy 
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier

kfold = KFold(n_splits=10, random_state=5)
c = DecisionTreeClassifier()
num_trees = 200
model = BaggingClassifier(base_estimator=c, n_estimators=num_trees, random_state=5)
model.fit(train[predictors],train[target])
results = cross_val_score(model, train[predictors], train[target], cv=kfold)
print(results.mean()) #0.675

results = cross_val_score(model, test[predictors], test[target], cv=kfold)
print(results.mean()) #0.633
# by trial and error method by changing random_state values and no of trees we get this optimum accuracy which is less

print(classification_report(test[target],pd.Series(model.predict(test[predictors])))) #0.74

#Adaboost classification
from sklearn.ensemble import AdaBoostClassifier

num_trees = 20
seed=10
kfold = KFold(n_splits=10, random_state=seed)
model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)
model.fit(train[predictors],train[target])
results = cross_val_score(model,train[predictors], train[target], cv=kfold)
print(results.mean())#0.692

results = cross_val_score(model,test[predictors], test[target], cv=kfold)
print(results.mean())#0.71 improved accuracy

print(classification_report(test[target],pd.Series(model.predict(test[predictors]))))#0.72

# Stacking Ensemble for Classification
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
kfold = KFold(n_splits=10, random_state=7)
# create the sub models
estimators = []
model1 = DecisionTreeClassifier()
estimators.append(('cart', model1))
model2 = SVC()
estimators.append(('svm', model2))

# create the ensemble model
ensemble = VotingClassifier(estimators)
ensemble.fit(train[predictors],train[target])
results = cross_val_score(ensemble, train[predictors], train[target], cv=kfold)
print(results.mean())#0.59

print(classification_report(test[target],pd.Series(ensemble.predict(test[predictors])))) #0.56

#########XG Boosting##############
from xgboost import XGBClassifier

# fit model no training data
model = XGBClassifier()
model.fit(train[predictors], train[target])

results = cross_val_score(model, train[predictors], train[target])
print(results.mean()) #0.64

results = cross_val_score(model,test[predictors], test[target])
print(results.mean()) #0.583

print(classification_report(test[target],pd.Series(ensemble.predict(test[predictors]))))#0.56

#Using SMOTE Algorithm Oversampling of minority class samples
from imblearn.over_sampling import SMOTE 
sm = SMOTE(random_state = 2)

X_train_res, Y_train_res = sm.fit_sample(train[predictors],train[target])

X_test_res,Y_test_res = sm.fit_sample(test[predictors],test[target])

Y_train_res.shape, Y_test_res.shape

plt.hist(Y_train_res) ; plt.hist(Y_test_res);

(pd.Series(Y_train_res)).value_counts()   #all classes have similar proportion

#XGBoost Model for oversampled data using smote
model= XGBClassifier()
model.fit(X_train_res, Y_train_res)

results = cross_val_score(model,X_train_res, Y_train_res)
print(results.mean()) #0.78

results = cross_val_score(model,X_test_res, Y_test_res)
print(results.mean())#0.817

print(classification_report(Y_test_res,pd.Series(model.predict(X_test_res))))#0.69

####By Smote technique we can oversample minority class and improve the model accuracy as well as precision ,recall for such classes
### XGboost model accuracy increased from 0.58 to 0.78 same we can perform on other models also to improve accuracy and other params

#Decrease the categories in Sales to 2 to balance the data 
#By Reducing Categories in Target as Binary Classifier
data_new["Sales"]=data["Sales"]

#make categorical data for sales by two classes and remove data set imbalance issue 
#making threshhold limit as 7
# 1 : Sales<=7
# 2 : Sales>7
for i in range(0,len(data)):
    if(data_new["Sales"][i]<=7):
        data_new["Sales"][i]="<=7"
    else:    
        data_new["Sales"][i]=">7"

data_new["Sales"].value_counts()

plt.hist(data_new["Sales"])

# Splitting data into training and testing data set by 70:30 ratio

from sklearn.model_selection import train_test_split
train,test = train_test_split(data_new,test_size = 0.3,random_state=5)

#Decision Tree
model = DecisionTreeClassifier(criterion = 'entropy')
model.fit(train[predictors],train[target])
preds = model.predict(test[predictors])
pd.Series(preds).value_counts()
pd.crosstab(test[target],preds)

# Accuracy = train  
np.mean(train.Sales == model.predict(train[predictors]))#1

# Accuracy = Test
np.mean(preds==test.Sales) # 0.74

print(classification_report(test[target],pd.Series(model.predict(test[predictors]))))

#Bagging Classifier to improve accuracy 
kfold = KFold(n_splits=10, random_state=5)
c = DecisionTreeClassifier()
num_trees = 200
model = BaggingClassifier(base_estimator=c, n_estimators=num_trees, random_state=5)
model.fit(train[predictors],train[target])
results = cross_val_score(model, train[predictors], train[target])
print(results.mean()) #0.825

results = cross_val_score(model, test[predictors], test[target])
print(results.mean()) #0.775

print(classification_report(test[target],pd.Series(model.predict(test[predictors]))))#0.85

#ADBoosting technique
num_trees = 20
seed=10
kfold = KFold(n_splits=10, random_state=seed)
model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)
model.fit(train[predictors],train[target])
results = cross_val_score(model,train[predictors], train[target], cv=kfold)
print(results.mean())#0.857

results = cross_val_score(model,test[predictors], test[target], cv=kfold)
print(results.mean())#0.8

#Stacking ensemble for classification
estimators = []
model1 = LogisticRegression(max_iter=100)
estimators.append(('logistic', model1))
model2 = DecisionTreeClassifier()
estimators.append(('cart', model2))
model3 = SVC()
estimators.append(('svm', model3))

# create the ensemble model
ensemble = VotingClassifier(estimators)
ensemble.fit(train[predictors],train[target])
results = cross_val_score(ensemble, train[predictors], train[target])
print(results.mean())#0.84

results = cross_val_score(ensemble,test[predictors], test[target])
print(results.mean())#0.74

print(classification_report(test[target],pd.Series(ensemble.predict(test[predictors]))))

#XGBoosting technique
# fit model no training data
model = XGBClassifier()
model.fit(train[predictors], train[target])

results = cross_val_score(model, train[predictors], train[target])
print(results.mean()) #0.814

results = cross_val_score(model,test[predictors], test[target])
print(results.mean())#0.82

print(classification_report(test[target],pd.Series(model.predict(test[predictors]))))#0.82

#from two classes model we choose XGboost as it has high accuracy #0.82 on test dataset